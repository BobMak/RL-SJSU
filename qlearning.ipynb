{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting\n"
     ]
    }
   ],
   "source": [
    "from utils import Agent, run_n_steps\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "class RandLander(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return env.action_space.sample()\n",
    "    def think(self, observation_old, action, reward, observation, terminated):\n",
    "        return super().think(observation_old, action, reward, observation, terminated)\n",
    "\n",
    "rand_agent = RandLander()\n",
    "\n",
    "cont = run_n_steps(env, rand_agent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few times to get a feel for the environment\n",
    "cont = run_n_steps(env, rand_agent, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanderNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetworkAgent(Agent):\n",
    "    def __init__(self) -> None:\n",
    "        self.network = LanderNetwork()\n",
    "    def act(self, observation, periphral=None):\n",
    "        obs = torch.from_numpy(observation)\n",
    "        action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def think(self, observation_old, action, reward, observation, terminated):\n",
    "        return super().think(observation_old, action, reward, observation, terminated)\n",
    "\n",
    "sna = SimpleNetworkAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few times to get a feel for the environment\n",
    "cont = run_n_steps(env, sna, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableNetworkAgent(Agent):\n",
    "    def __init__(self, gamma = 0.9, lr=0.001) -> None:\n",
    "        self.network = LanderNetwork()\n",
    "        self.gamma = gamma\n",
    "        self.optim = torch.optim.AdamW(self.network.parameters(), lr)\n",
    "    def act(self, observation, periphral=None):\n",
    "        obs = torch.from_numpy(observation)\n",
    "        with torch.no_grad():\n",
    "            action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def think(self, observation, action, reward, observation_next, terminated):\n",
    "        cr_pred = self.network(observation)[action]\n",
    "        cr_esti = reward + self.gamma*self.network(observation_next)\n",
    "        loss = (cr_esti - cr_pred)**2\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that takes a model and an environment, and run the model in the environment for *n* steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def act(self, observation, periphral=None):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        pass\n",
    "\n",
    "def run_n_steps(env, agent: Agent, n, continuation=None):\n",
    "    if continuation is not None:\n",
    "        observation, reward, terminated, truncated, info = continuation\n",
    "    if continuation is None or terminated or truncated:\n",
    "        print(\"Resetting\")\n",
    "        observation, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "    step = 0\n",
    "    # print((observation, reward, terminated, truncated, info))\n",
    "    while not terminated and not truncated and step < n:\n",
    "        action = agent.act(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.think(observation_old, action, reward, observation)\n",
    "        step += 1\n",
    "    return (observation, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "class RandomLunar(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return env.action_space.sample()\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        return super().think(observation_old, action, reward, observation)\n",
    "randAgent = RandomLunar()\n",
    "\n",
    "\n",
    "cont = run_n_steps(env, randAgent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = run_n_steps(env, randAgent, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
