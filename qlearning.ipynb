{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7982ef5e4760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xkcd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanderNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Network Agent\n",
    "--------------------\n",
    "This is just a simple agent that uses the network but does not train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent\n",
    "class SimpleNetworkAgent(Agent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "    def act(self, observation, periphral=None):\n",
    "        obs = torch.from_numpy(observation)\n",
    "        action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def think(self, observation_old, action, reward, observation, terminated):\n",
    "        return super().think(observation_old, action, reward, observation, terminated)\n",
    "\n",
    "sna = SimpleNetworkAgent()\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "\n",
    "for round in trange(20):\n",
    "    cont = run_upto_n_steps(env, sna, 150, cont)\n",
    "    if round % 2 == 0:\n",
    "        plt.clf()\n",
    "        plot_reward_and_episodes(sna)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:34<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "\n",
    "for round in trange(200):\n",
    "    cont = run_upto_n_steps(env, sna, 150, cont)\n",
    "    if round % 2 == 0:\n",
    "        plt.clf()\n",
    "        plot_reward_and_episodes(sna)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable Network Agent\n",
    "-----------------------\n",
    "This agent will train the network by optimizing on the most recent few \n",
    "(Observaation, Action, Reward, Observation) quartuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Agent\n",
    "class TrainableNetworkAgent(Agent):\n",
    "    def __init__(self, gamma = 0.9, lr=0.001, update_interval=32, epsilon=0.1, terminal_multiplier=1) -> None:\n",
    "        super().__init__()\n",
    "        self.network = LanderNetwork()\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.epsilon = epsilon\n",
    "        self.terminal_multiplier = terminal_multiplier\n",
    "        self.optim = torch.optim.AdamW(self.network.parameters(), lr)\n",
    "        self.loss = torch.tensor(0., requires_grad=True)\n",
    "        self.thoughts = 0\n",
    "        self.losses = []\n",
    "        self.events = [] # List of dictionary of events\n",
    "    def act(self, observation, periphral=None):\n",
    "        if self.network.training and torch.rand((1,)).item() < self.epsilon:\n",
    "            return torch.randint(self.network.layers[-1].out_features, (1,)).item()\n",
    "        obs = torch.from_numpy(observation)\n",
    "        with torch.no_grad():\n",
    "            action = self.network(obs)\n",
    "        return action.argmax().item()\n",
    "    def think(self, observation, action, reward, observation_next, terminated):\n",
    "        if not self.network.training:\n",
    "            return \n",
    "        self.thoughts += 1\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            observation = torch.from_numpy(observation)\n",
    "        if isinstance(observation_next, np.ndarray):\n",
    "            observation_next = torch.from_numpy(observation_next)\n",
    "        outputs = self.network(observation)\n",
    "        next_outputs = self.network(observation_next)\n",
    "        cr_pred = outputs[action]\n",
    "        cr_esti = reward + self.gamma*next_outputs.argmax()\n",
    "\n",
    "        loss = (cr_esti - cr_pred)**2\n",
    "        if terminated:\n",
    "            loss = loss + self.terminal_multiplier*sum(next_outputs**2)\n",
    "        self.loss = self.loss + loss\n",
    "        self.losses.append(loss.item())\n",
    "        self.events.append({\n",
    "            \"terminal\": terminated,\n",
    "            \"reward\": reward\n",
    "        })\n",
    "        if self.thoughts % self.update_interval == 0:\n",
    "            self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        self.optim.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.loss = torch.tensor(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna = TrainableNetworkAgent(terminal_multiplier=1, gamma=0.99)\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:15<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import run_upto_n_steps, plot_reward_and_episodes\n",
    "from tqdm import trange\n",
    "tna.network.train()\n",
    "for round in trange(2000):\n",
    "    try:\n",
    "        cont = run_upto_n_steps(env, tna, 150, cont)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    if round % 20 == 0:\n",
    "        # plot_agent(tna)\n",
    "        plt.clf()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.scatter(*zip(*enumerate(tna.losses)), c=[i['terminal'] for i in tna.events], s=10, alpha=0.8)\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plot_reward_and_episodes(tna)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:02<00:00, 12.49s/it]\n"
     ]
    }
   ],
   "source": [
    "enviz = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "tna.network.eval()\n",
    "for i in trange(5):\n",
    "    run_upto_n_steps(enviz, tna, 1_000)\n",
    "input()\n",
    "enviz.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import numpy as np\n",
    "class RandLander(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return np.random.choice([0,1,2,3])\n",
    "    def think(self, observation_old, action, reward, observation, terminated):\n",
    "        pass\n",
    "\n",
    "rand_agent = RandLander()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Agent, run_upto_n_steps\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# logging.basicConfig(level=logging.INFO) # If your interested in some logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(env, rand_agent, 50, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "cont = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_and_plot\n",
    "\n",
    "for round in range(100):\n",
    "    cont = run_and_plot(env, rand_agent, 150, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that takes a model and an environment, and run the model in the environment for *n* steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def act(self, observation, periphral=None):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        pass\n",
    "\n",
    "def run_n_steps(env, agent: Agent, n, continuation=None):\n",
    "    if continuation is not None:\n",
    "        observation, reward, terminated, truncated, info = continuation\n",
    "    if continuation is None or terminated or truncated:\n",
    "        print(\"Resetting\")\n",
    "        observation, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "    step = 0\n",
    "    # print((observation, reward, terminated, truncated, info))\n",
    "    while not terminated and not truncated and step < n:\n",
    "        action = agent.act(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.think(observation_old, action, reward, observation)\n",
    "        step += 1\n",
    "    return (observation, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "class RandomLunar(Agent):\n",
    "    def act(self, observation, periphral=None):\n",
    "        return env.action_space.sample()\n",
    "    def think(self, observation_old, action, reward, observation):\n",
    "        return super().think(observation_old, action, reward, observation)\n",
    "randAgent = RandomLunar()\n",
    "\n",
    "\n",
    "cont = run_n_steps(env, randAgent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = run_n_steps(env, randAgent, 10, cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
